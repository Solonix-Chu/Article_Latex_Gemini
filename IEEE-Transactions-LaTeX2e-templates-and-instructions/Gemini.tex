\documentclass[journal,twocolumn]{IEEEtran}

%
% Packages:
%
\usepackage{amsmath,amssymb,amsfonts} % For mathematical symbols and environments
\usepackage{algorithmic} % For algorithms (if any, or can be removed)
\usepackage{graphicx} % For including figures (placeholders will be used)
\usepackage{textcomp} % For additional text symbols
\usepackage{xcolor} % For colors (if needed, or can be removed)
\usepackage{cite} % For bibliography management
\usepackage{balance} % To balance columns on the last page
\usepackage{hyperref} % For clickable links, optional

%
% Correct bad hyphenation here
%
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% Paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Reinforcement Learning for Channel Estimation in Communications: A Comprehensive Review}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph of footnote material
\author{Gemini% <-this % stops a space
}

% note the % following the last \IEEEmembership and also \thanks -
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
%
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not notice if the said evil space
% somehow managed to creep in.



% The paper headers
% \markboth{Journal of Selected Topics in Signal Processing,~Vol.~XX, No.~Y, Month~2025}%
% {Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
%
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2025 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
Accurate channel state information (CSI) is crucial for modern wireless systems, but increasing complexity and dynamics pose significant challenges to traditional estimation techniques. Reinforcement Learning (RL) offers a promising solution by learning optimal strategies through environmental interaction without explicit models. This review examines RL applications in channel estimation, focusing on algorithms like Q-learning, DQN, DDPG, and PPO. We highlight key achievements in optimizing IRS configurations, estimating time-varying channels, facilitating MIMO systems, and enabling end-to-end communication. RL-based methods demonstrate superior performance over conventional approaches in terms of reduced estimation error and improved system-level metrics. While acknowledging challenges like computational complexity and generalization issues, we explore future directions including meta-RL, lightweight algorithms, and holistic optimization approaches. This review aims to advance understanding of RL's role in addressing channel estimation challenges for next-generation wireless networks.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Reinforcement Learning, Channel Estimation, Wireless Communications, Deep Learning, Machine Learning, 5G, 6G, Intelligent Reflecting Surface (IRS), MIMO, Dynamic Channels, End-to-End Learning.
\end{IEEEkeywords}






% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
% IEEEtranNWS [showrules] V1.8b enabled.

Modern wireless communications increasingly rely on accurate Channel State Information (CSI) as systems evolve toward 5G/6G with massive MIMO, mmWave/THz frequencies, and Intelligent Reflecting Surfaces (IRS). Traditional estimation methods face significant challenges in these complex scenarios: massive MIMO systems create high-dimensional channel matrices; vehicular and high-speed scenarios produce rapidly varying channels where quasi-static assumptions fail; mmWave sensitivity and IRS-created cascaded channels complicate modeling; and increasing pilots for better estimation reduces spectral efficiency. Reinforcement Learning (RL) offers a compelling solution by learning optimal policies through environmental interaction without explicit models or labeled data, making it particularly suitable for complex wireless channels. RL agents learn to adaptively select pilots, optimize IRS configurations, denoise estimates, track variations, and even optimize end-to-end communication parameters. Deep Reinforcement Learning (DRL), combining RL with neural networks, effectively handles the high-dimensional spaces typical in wireless communications. This review explores RL applications in channel estimation, examining methodologies, research achievements across diverse scenarios, performance improvements over conventional approaches, inherent challenges, and promising future directions.

\section{Core Reinforcement Learning Methodologies for Channel Estimation}
Several RL algorithms and frameworks are being explored for channel estimation. These can be broadly categorized based on how they learn and represent the policy and/or value function. Table \ref{tab:rl_algorithms_summary} (placeholder for a table that could summarize these) would typically summarize these. For the purpose of this textual report, we discuss them below.

\subsection{Value-Based Methods}
Value-based methods learn a value function that estimates the expected return for each state or state-action pair. The policy is then derived implicitly by selecting actions that maximize this value function.

\subsubsection{Q-Learning}
Q-learning is a model-free, off-policy RL algorithm that aims to learn the optimal action-value function, $Q^*(s, a)$, which represents the maximum expected future reward achievable by taking action $a$ in state $s$ and following the optimal policy thereafter. The Q-values are typically stored in a table (Q-table) for discrete state and action spaces and updated iteratively using the Bellman equation:
$Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha [r_{t+1} + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t)]$
where $\alpha$ is the learning rate and $\gamma$ is the discount factor.
In channel estimation, Q-learning has been applied to tasks like channel prediction in NOMA systems [41] and successive denoising in MIMO-OFDM systems [42] where the agent learns to select optimal actions (e.g., predicted channel coefficients, subcarriers to denoise) based on the current channel state representation.

\subsubsection{Deep Q-Networks (DQN)}
For problems with large or continuous state spaces, Q-tables become impractical. DQN addresses this by using a DNN to approximate the Q-function, $Q(s, a; \theta)$, where $\theta$ represents the network weights. DQNs often employ techniques like experience replay (storing and randomly sampling past transitions to break correlations) and target networks (using a separate, periodically updated network to stabilize learning targets) to improve stability and performance.
DQN and its variants (e.g., Double DQN - DDQN) are relevant for channel estimation in IoT backscatter communications [28] and can be applied to optimize parameters related to channel adaptation.

\subsection{Policy Gradient and Actor-Critic Methods: Advanced Strategies}
Policy gradient methods directly learn the policy function $\pi(a|s; \theta)$, which maps states to actions (or probabilities of actions). The policy parameters $\theta$ are updated by performing gradient ascent on an objective function that measures the expected cumulative reward.

Actor-Critic methods combine the strengths of value-based and policy-based approaches. They consist of two components:
\begin{itemize}
    \item \textbf{Actor:} Learns and implements the policy $\pi(a|s; \theta)$.
    \item \textbf{Critic:} Learns a value function (e.g., state-value function $V(s; \phi)$ or action-value function $Q(s,a; \phi)$) to evaluate the actions taken by the actor. The critic's evaluations are then used to guide the actor's policy updates.
\end{itemize}

\subsubsection{Deep Deterministic Policy Gradient (DDPG)}
DDPG is an actor-critic, model-free algorithm designed for continuous action spaces. It learns a deterministic policy (Actor) and an action-value function (Critic). DDPG utilizes experience replay and target networks similar to DQN. It has been successfully applied to:
\begin{itemize}
    \item Optimizing IRS configurations, where the action space (phase shifts) is continuous [7].
    \item End-to-end communication systems, where the transmitter (actor) learns to encode signals without explicit channel knowledge, using feedback from the receiver as a reward [11].
\end{itemize}

\subsubsection{Proximal Policy Optimization (PPO)}
PPO is another popular actor-critic algorithm that aims for more stable and reliable policy updates than vanilla policy gradient methods. It achieves this by using a clipped surrogate objective function that limits the size of policy changes at each step, preventing performance collapse. PPO is often easier to implement and tune.
While not always directly estimating channel coefficients, PPO is used for optimizing parameters in systems that rely on channel state, such as resource allocation in RIS-assisted MEC systems [16] and channel sampling patterns in ISAC [44]. It's also listed as a DRL algorithm for enhancing channel estimation in IoT backscatter systems by adapting policies [28].

\subsubsection{Twin Delayed Deep Deterministic Policy Gradient (TD3)}
TD3 is an extension of DDPG that addresses the overestimation bias of the critic and improves stability. It employs three key techniques: clipped double Q-learning (uses two critic networks and takes the minimum of their Q-values), delayed policy updates (updates the actor less frequently than the critic), and target policy smoothing (adds noise to the target action).
TD3 has been applied in RIS-assisted networks for optimizing the phase-shift matrix [20] and for signal detection in RIS-assisted ambient backscatter communication [20].

These algorithms provide a toolbox for tackling various aspects of channel estimation, from direct estimation and refinement to optimizing auxiliary systems that influence channel quality. The choice of algorithm often depends on the nature of the state and action spaces (discrete vs. continuous), the complexity of the problem, and the desired trade-off between sample efficiency and computational cost.

% Placeholder for Table 1
% \begin{table*}[!t]
% \renewcommand{\arraystretch}{1.3}
% \caption{Summary of Key RL Algorithms and their Relevance to Channel Estimation}
% \label{tab:rl_algorithms_summary}
% \centering
% \begin{tabular}{|l|l|l|l|l|}
% \hline
% \textbf{Algorithm Category} & \textbf{Algorithm} & \textbf{Key Idea} & \textbf{State/Action Space} & \textbf{Channel Estimation Application Examples} \\
% \hline
% Value-Based & Q-Learning & Learns Q-values in a table & Discrete & Channel prediction, subcarrier denoising \\
% \hline
% Value-Based & DQN & Uses DNN to approximate Q-function & Continuous state, discrete action & IRS parameter tuning, IoT policy adaptation \\
% \hline
% Policy Gradient / & DDPG & Actor-Critic for continuous actions & Continuous state \& action & IRS phase shift optimization, E2E communication \\
% Actor-Critic & & & & \\
% \hline
% Policy Gradient / & PPO & Clipped surrogate objective for stable updates & Continuous or Discrete & RIS/IoT parameter optimization, channel sampling \\
% Actor-Critic & & & & \\
% \hline
% Policy Gradient / & TD3 & Addresses DDPG's overestimation bias & Continuous state \& action & RIS phase shift optimization, signal detection \\
% Actor-Critic & & & & \\
% \hline
% \end{tabular}
% \end{table*}


\section{Reinforcement Learning in Action: Channel Estimation Across Diverse Communication Scenarios}
RL techniques are being increasingly explored to address channel estimation challenges in a variety of modern wireless communication contexts.

\subsection{Intelligent Reflecting Surfaces (IRS) / Reconfigurable Intelligent Surfaces (RIS)}
IRS/RIS technology, which uses passive elements to reflect and steer incident signals, can create programmable wireless environments. However, optimizing the large number of phase shifts at the IRS elements, especially with limited or no dedicated sensing capabilities at the IRS, is a complex task that involves understanding and manipulating the cascaded channel (Base Station-IRS-User).
RL, particularly DRL algorithms like DDPG [7], DQN, PPO, and TD3 [20], has shown significant promise in dynamically optimizing IRS phase shifts.
\begin{itemize}
    \item \textbf{Problem Formulation:} The RL agent (often at the BS or a network controller) observes the state (e.g., received signal quality, estimated CSI, user locations) and takes actions (adjusting IRS phase shifts). The reward is typically based on metrics like sum-rate, Signal-to-Noise Ratio (SINR), coverage, or energy efficiency.
    \item \textbf{Achievements:} Studies show that RL can adaptively optimize IRS configurations in dynamic environments, improving signal quality, coverage, and overall system performance compared to traditional optimization or random phase shifts [7, 20]. RL can learn to overcome signal blockages and enhance weak signal paths.
    \item \textbf{Channel Estimation Aspect:} While RL primarily optimizes the IRS configuration, this process is intrinsically linked to the channel. The agent learns a policy that implicitly accounts for the channel characteristics to achieve its objective. Some research also looks at using RL to aid in the explicit estimation of the complex cascaded channels in IRS systems [7].
\end{itemize}

\subsection{Time-Varying and Doubly Dispersive Channels}
In high-mobility scenarios (e.g., V2X, high-speed rail), wireless channels exhibit rapid variations in both time and frequency domains (doubly dispersive channels). Traditional channel estimation methods assuming quasi-static channels perform poorly.
RL offers a framework for adaptive channel estimation and tracking in such dynamic environments.
\begin{itemize}
    \item \textbf{Approach:} RL agents can be trained to select optimal detected data symbols for data-aided channel estimation, adapting to the channel's time-varying nature [8, 35]. The MDP formulation involves states representing historical channel information and detected symbols, actions related to selecting symbols for estimation, and rewards based on estimation accuracy (e.g., minimizing MSE).
    * Some works propose custom RL algorithms with state element refinement to explicitly capture channel variations [8].
    \item \textbf{Performance:} RL-aided estimators have demonstrated improved BER and NMSE compared to conventional pilot-aided methods and RL estimators designed for time-invariant channels, especially as channel variation increases [8].
\end{itemize}

\subsection{MIMO and Massive MIMO Systems}
The high dimensionality of channel matrices in MIMO and especially massive MIMO systems presents a significant estimation challenge, increasing pilot overhead and computational complexity.
\begin{itemize}
    \item \textbf{RL for Symbol Selection/Denoising:} Q-learning has been used for successive channel denoising in MIMO-OFDM systems by formulating the selection of subcarriers for denoising as an MDP. The agent learns to identify and refine unreliable channel estimates based on channel curvature, approaching LMMSE performance without prior channel statistics [42]. Low-complexity RL algorithms have also been developed for selecting detected data symbols to aid channel estimation, reducing complexity while improving BER and NMSE [34].
    \item \textbf{Implicit Channel Handling in Resource Allocation:} While not always direct channel estimation, RL is used for resource allocation (e.g., beamforming, power control) in massive MIMO, where the agent learns to make decisions based on observed CSI or quality metrics, implicitly adapting to the high-dimensional channel environment.
\end{itemize}

\subsection{Beyond Explicit Models: RL in End-to-End (E2E) Communication Systems}
A more radical approach involves using DRL to learn the entire communication system (transmitter and receiver) in an end-to-end manner, without relying on traditional block-based designs or explicit channel models.
\begin{itemize}
    \item \textbf{DDPG-E2E Approach:} The DDPG-E2E framework treats the transmitter as an RL agent (actor) that learns to encode messages into signals [11, 12]. The wireless channel is an unknown environment. The receiver decodes the message, and the end-to-end performance (e.g., BER, reconstruction loss) is used to generate a reward signal for the transmitter.
    \item \textbf{Implicit Channel Estimation:} The transmitter and receiver DNNs jointly learn to adapt their strategies to the unknown channel characteristics through the E2E training process. The channel is effectively treated as a "black box" that the system learns to communicate over.
    \item \textbf{Benefits:} This approach can potentially discover novel communication schemes optimized for specific unknown channels and can be more robust to channel model imperfections. It eliminates the need for a separate channel estimation module.
    \item \textbf{Performance:} DDPG-E2E has shown better BLER performance and convergence rates compared to state-of-the-art solutions over complex wireless channels (e.g., Rician, Rayleigh) [11].
\end{itemize}

\subsection{Emerging Frontiers and Specific Scenarios}
RL is also finding applications in channel estimation or related optimization for other emerging wireless scenarios:

\subsubsection{Millimeter-Wave (mmWave) Communications}
MmWave systems, while offering large bandwidth, suffer from high path loss and sensitivity to blockage. Beamforming is crucial, and accurate CSI is needed.
\begin{itemize}
    \item \textbf{RL for Beamforming and ADC Optimization:} RL (e.g., policy gradient methods) is used to jointly optimize hybrid beamforming matrices and ADC threshold levels in mmWave MIMO systems with low-resolution ADCs. The RL agent learns to maximize achievable rates by adapting to channel statistics (implicitly handled via SNR variations during training) and can be robust to noisy CSI estimates [45].
\end{itemize}

\subsubsection{Internet of Things (IoT) and Backscatter Communications}
For low-power IoT devices and backscatter communication systems, efficient channel estimation and adaptation are critical.
\begin{itemize}
    \item \textbf{DRL for Adaptive Policies:} A survey on DRL for backscatter communications highlights that DRL (including DQN, DDQN, DDPG, PPO) can enhance channel estimation by enabling devices to adapt their transmission parameters (modulation, power) and estimation policies based on changing SNR and interference, leading to more accurate channel estimates and improved system performance [28]. RL also optimizes resource allocation, packet scheduling, and radio access network selection, all of which rely on or interact with channel conditions.
\end{itemize}

\subsubsection{Vehicular-to-Everything (V2X) Communications}
V2X channels are characterized by high mobility, rapid variations, and non-stationarity.
\begin{itemize}
    \item \textbf{RL for Resource Management and Adaptation:} Most current RL research in V2X focuses on resource allocation, spectrum sharing, and adaptive beamforming/transmission strategies to cope with dynamic channel conditions [30, 31, 32]. For example, DRL is used to optimize beamforming and power allocation in ISAC for V2X, reducing reliance on extensive pilot signals by using sensing state information [32]. While not always estimating channel coefficients directly with RL, these systems learn to operate effectively in challenging V2X channels.
\end{itemize}

\subsubsection{Underwater Acoustic Communications (UWA)}
UWA channels are notorious for long multipath delays, significant Doppler shifts, and limited bandwidth, making channel estimation extremely difficult.
\begin{itemize}
    \item \textbf{RL for Adaptive Systems:} DRL (e.g., LSTM-DQN) has been applied to adaptive modulation in UWA communications to select appropriate modulation modes based on outdated or predicted CSI, outperforming traditional methods [39]. Some research mentions model-based RL frameworks to recursively estimate channel model parameters and track dynamics [39].
\end{itemize}
Across these diverse scenarios, RL provides a flexible and adaptive framework to tackle the intricacies of channel estimation, either directly or by optimizing systems to work efficiently with available channel information.

\section{Notable Achievements and Performance Benchmarks of RL in Channel Estimation}
The application of Reinforcement Learning to channel estimation has yielded several notable achievements, often demonstrating superior performance compared to traditional methods or even other machine learning approaches in specific contexts. Key performance improvements are typically measured in terms of:

\begin{itemize}
    \item \textbf{Reduced Estimation Error:} RL-based methods have shown significant reductions in Normalized Mean Square Error (NMSE) or Mean Square Error (MSE) of channel estimates.
        \begin{itemize}
            \item For IRS-assisted systems, DRL approaches combining CNNs and GRUs with DDPG for IRS phase shift optimization have achieved markedly smaller channel estimation errors (NMSE) for both direct and cascaded channels compared to traditional LS and LMMSE methods across public datasets and real test scenarios [7].
            \item In MIMO-OFDM systems, Q-learning based successive denoising methods have approached the MSE performance of ideal LMMSE (which requires perfect channel statistics) and offered substantial gains (e.g., ~6 dB) over LS estimation [42].
            \item Low-complexity RL algorithms for selecting detected symbols in MIMO systems have significantly reduced NMSE compared to conventional LMMSE [34].
        \end{itemize}
    \item \textbf{Improved System-Level Performance:}
        \begin{itemize}
            \item \textbf{Bit Error Rate (BER) / Block Error Rate (BLER):} Lower channel estimation error translates to improved demodulation and decoding.
                \begin{itemize}
                    \item RL-aided channel estimators for time-varying MIMO systems have demonstrated better BLER than conventional pilot-aided methods and RL estimators designed for time-invariant channels [8, 35].
                    \item Low-complexity RL estimators for MIMO systems achieved BER improvements of approximately 0.7-1.2 dB at a BLER of $10^{-1}$ compared to conventional methods [34].
                    \item DDPG-E2E systems have shown better BLER performance and convergence rates over complex wireless channels compared to state-of-the-art E2E learning solutions [11].
                \end{itemize}
            \item \textbf{Achievable Rate / Sum-Rate / Throughput:} More accurate CSI enables more effective resource allocation, beamforming, and interference management.
                \begin{itemize}
                    \item Q-learning for channel prediction in MISO-NOMA systems has enhanced the sum rate compared to standard MMSE and DL-based LSTM procedures [41].
                    \item DRL-optimized IRS configurations have led to higher achievable system rates after beamforming co-optimization [7].
                    \item RL-based optimization of beamforming and ADC thresholds in mmWave MIMO systems has closely matched exhaustive search performance and significantly outperformed traditional baselines in terms of achievable rates [45].
                \end{itemize}
        \end{itemize}
    \item \textbf{Enhanced Robustness and Adaptability:}
        \begin{itemize}
            \item RL agents can adapt to dynamic channel environments where traditional methods struggle. This is particularly evident in time-varying channels [8, 35] and systems with IRS where the environment can be actively shaped [7].
            \item RL-based approaches can operate without perfect prior channel knowledge or extensive pre-labeled datasets, which is a significant advantage over some supervised DL techniques [42].
            \item DDPG-E2E systems demonstrate the ability to jointly train transmitters and receivers over unknown channels, showcasing adaptability [11].
        \end{itemize}
    \item \textbf{Reduced Overhead or Complexity (in some cases):}
        \begin{itemize}
            \item By learning to select optimal data symbols or by enabling end-to-end learning, RL can potentially reduce the reliance on extensive pilot signaling.
            \item Research into low-complexity RL algorithms aims to make these solutions practical for deployment by reducing computational demands compared to initial, more complex RL proposals [34]. For example, by using sub-blocks and backup samples, computational complexity and latency were significantly reduced without performance loss in some RL-aided channel estimators [35].
        \end{itemize}
\end{itemize}

While quantitative comparisons vary widely depending on the specific scenario, RL algorithm, and baseline methods, the trend indicates that RL holds significant potential for pushing the boundaries of channel estimation performance, particularly in complex and dynamic next-generation wireless systems. The ability to learn and adapt from interaction provides a powerful alternative to model-based approaches that may falter when their assumptions are violated.

\section{Overcoming Hurdles: Challenges and Limitations of RL Application in Channel Estimation}
Despite the promising results, the practical application of Reinforcement Learning in channel estimation is not without its challenges and limitations. Addressing these is crucial for realizing the full potential of RL in real-world wireless systems.

\begin{itemize}
    \item \textbf{Computational Complexity and Latency:}
        \begin{itemize}
            \item \textbf{Training Complexity:} DRL algorithms, especially those involving deep neural networks (e.g., DQN, DDPG, PPO), can be computationally intensive to train, requiring significant data (interactions with the environment) and processing power. This can be a bottleneck for rapidly changing channel environments or resource-constrained devices [34, 35, 20].
            \item \textbf{Inference Latency:} Even after training, the inference time of a complex DRL agent might be too high for real-time channel estimation, which often has stringent latency requirements.
            \item \textbf{Q-table Size:} Traditional Q-learning is limited by the "curse of dimensionality," where the size of the Q-table grows exponentially with the state and action spaces, making it infeasible for many practical channel estimation problems [41]. DRL mitigates this but introduces neural network complexity.
        \end{itemize}
    \item \textbf{Sample Efficiency and Convergence Speed:}
        \begin{itemize}
            \item RL agents often require a large number of interactions with the environment (samples) to learn an effective policy. In wireless communication, this can translate to a long training time or the need for extensive pilot transmissions or feedback, potentially impacting spectral efficiency.
            \item Convergence to an optimal or near-optimal policy can be slow, especially in complex environments with sparse rewards or long episodes.
        \end{itemize}
    \item \textbf{Adaptation to Highly Dynamic and Non-Stationary Environments:}
        \begin{itemize}
            \item While RL is designed for dynamic environments, extremely fast-changing or non-stationary channel conditions can still pose a challenge. A policy learned under one set of channel statistics might not perform well if the statistics change drastically and rapidly [8, 35].
            \item Potential delays in dynamic decision-making by RL agents can affect real-time performance in fast-fading channels [7].
        \end{itemize}
    \item \textbf{Reward Function Design:}
        \begin{itemize}
            \item Designing an appropriate reward function that accurately reflects the channel estimation objective (e.g., minimizing NMSE, maximizing system throughput) and guides the agent effectively is non-trivial.
            \item Sparse or delayed rewards can make learning difficult. For instance, the impact of a channel estimation action on the final BER might only be observable after many subsequent steps.
        \end{itemize}
    \item \textbf{Generalization and Robustness:}
        \begin{itemize}
            \item A policy trained in a specific simulation environment or under certain channel conditions might not generalize well to different, unseen real-world scenarios.
            \item RL agents can be sensitive to hyperparameters, and finding the right set often requires extensive tuning.
            \item Performance saturation at high SNRs has been observed, where models cannot achieve estimation errors infinitely close to zero [7].
        \end{itemize}
    \item \textbf{State and Action Space Definition:}
        \begin{itemize}
            \item Defining appropriate state and action spaces that capture relevant information without being excessively large is critical. Continuous or very large discrete action spaces (e.g., fine-grained IRS phase shifts) can be challenging for some RL algorithms, though methods like DDPG are designed for continuous actions.
        \end{itemize}
    \item \textbf{Exploration vs. Exploitation Trade-off:}
        \begin{itemize}
            \item The agent needs to balance exploring new actions to discover better policies and exploiting known good actions to maximize immediate rewards. Inefficient exploration can lead to suboptimal policies or slow convergence.
        \end{itemize}
    \item \textbf{Challenges Specific to Certain Applications:}
        \begin{itemize}
            \item \textbf{IRS Systems:} Continuously acquiring accurate CSI for effective IRS optimization in dynamic environments is a challenge. Optimal IRS placement and the power consumption of active/mobile RIS are also concerns [20].
            \item \textbf{Data-Aided Estimation:} Using detected data symbols for estimation can lead to error propagation if symbols are detected incorrectly [8, 34]. RL helps in selective exploitation, but the risk remains.
            \item \textbf{End-to-End Systems:} While promising, understanding the learned policies and ensuring interpretability in DDPG-E2E systems can be difficult. Training these systems can be computationally intensive [12].
        \end{itemize}
    \item \textbf{Security and Vulnerability:}
        \begin{itemize}
            \item Like other machine learning models, DRL-based channel estimation systems can be vulnerable to adversarial attacks, where malicious inputs are crafted to degrade estimation performance or compromise the system [29].
        \end{itemize}
\end{itemize}
Addressing these limitations through algorithmic improvements, more efficient learning paradigms (like meta-learning or transfer learning), and careful system design is key to the widespread adoption of RL for channel estimation.

\section{The Path Forward: Innovative Frontiers and Research Directions for RL in Channel Estimation}
The application of Reinforcement Learning to channel estimation is a vibrant research area with numerous exciting avenues for innovation. Future efforts are likely to focus on enhancing the intelligence, efficiency, robustness, and practicality of RL-based solutions.

\subsection{Advanced Learning Paradigms}
\subsubsection{Meta-Reinforcement Learning (Meta-RL): Enabling Rapid Adaptation}
Wireless channels are often non-stationary and can change significantly based on the environment, user mobility, or frequency band. Meta-RL, or "learning to learn," aims to train agents that can quickly adapt to new channel environments or tasks with minimal additional training data.
\begin{itemize}
    \item \textbf{Concept:} The agent is trained over a distribution of different (but related) channel estimation tasks, learning a meta-policy or a good initial set of parameters that can be rapidly fine-tuned for a new, unseen channel environment.
    \item \textbf{Applications in CE:} Model-Agnostic Meta-Learning (MAML) and its variants are being explored for time-varying OFDM channel estimation [52, 54], allowing networks to learn channel characteristics and quickly adapt to new tasks with few training samples. Meta-DRL (e.g., DQN + MAML) is also proposed for optimizing UAV operations which inherently depend on channel conditions [48].
    \item \textbf{Benefits:} Reduced sample complexity for new environments, faster convergence, and better generalization.
\end{itemize}

\subsubsection{Transfer Learning: Leveraging Prior Knowledge}
Transfer learning allows knowledge gained from a source task/environment to be applied to a different but related target task/environment.
\begin{itemize}
    \item \textbf{Concept:} An RL agent pre-trained on a general set of channel conditions or a simulated environment can be fine-tuned for a specific, new deployment scenario, requiring less data and time than training from scratch.
    \item \textbf{Applications in CE:} Deep Transfer Learning (DTL) is used for downlink channel prediction in FDD massive MIMO systems, where models trained on previous environments are fine-tuned for new ones [49]. It's also proposed to accelerate the convergence of Heterogeneous Federated Learning (HFL) for channel estimation by enhancing local model parameter initialization [50].
    \item \textbf{Benefits:} Faster learning in new environments, improved performance with limited data.
\end{itemize}

\subsubsection{Federated and Distributed Reinforcement Learning: Towards Privacy-Preserving and Scalable Channel Estimation}
With growing concerns about data privacy and the distribution of intelligence to the edge, Federated Reinforcement Learning (FRL) and distributed RL are gaining traction.
\begin{itemize}
    \item \textbf{Concept:} Multiple agents (e.g., user devices, edge nodes) collaboratively train a global RL model without sharing their raw local channel data. Each agent trains a local model based on its own channel observations and interactions, and then model updates (parameters or gradients) are aggregated centrally (or in a decentralized manner) to improve the global model.
    \item \textbf{Applications in CE:} FRL is explored for channel estimation in RIS-assisted cell-free MIMO systems, where DRL (e.g., Qmix) is used for coalition formation among users to improve accuracy and reduce overhead, with transfer learning accelerating convergence [50]. Distributed learning-based channel estimation models are seen as candidates for future wireless systems beyond 5G [4].
    \item \textbf{Benefits:} Enhanced data privacy, reduced communication overhead (compared to sending raw data), improved scalability, and robustness due to learning from diverse data sources.
\end{itemize}

\subsection{Lightweight and Computationally Efficient DRL Algorithms}
For practical deployment, especially on resource-constrained devices (e.g., IoT devices, mobile terminals), the computational complexity and energy consumption of DRL agents must be minimized.
\begin{itemize}
    \item \textbf{Research Focus:} Developing lightweight DRL architectures and algorithms through techniques such as:
        \begin{itemize}
            \item \textbf{Network Pruning and Quantization:} Reducing the size and complexity of the neural networks used in DRL.
            \item \textbf{Knowledge Distillation:} Training smaller "student" networks to mimic the behavior of larger, pre-trained "teacher" networks.
            \item \textbf{Efficient Exploration Strategies:} Reducing the number of samples needed for learning.
            \item \textbf{Low-Complexity RL Algorithms:} Designing RL algorithms that are inherently less computationally demanding, such as the low-complexity RL for symbol selection in MIMO systems which divides data blocks into sub-blocks and uses partial soft information [34].
        \end{itemize}
    \item \textbf{Goal:} To make DRL-based channel estimation feasible for on-device learning and real-time operation in a wider range of scenarios.
\end{itemize}

\subsection{Robust Reinforcement Learning Agents}
Real-world wireless channels can be adversarial, with unpredictable interference, jamming, or sudden changes.
\begin{itemize}
    \item \textbf{Concept:} Designing RL agents that are robust to uncertainties, disturbances, and even adversarial attacks on the learning process or the input data (e.g., pilot signals). This involves techniques from robust optimization and game theory.
    \item \textbf{Applications in CE:} RL combined with IRS can inherently improve robustness by adaptively reconfiguring the environment [7]. Secure channel estimation using AI/DL, considering adversarial attacks, is an active area [29].
    \item \textbf{Goal:} To ensure reliable performance of RL-based channel estimators even in challenging and potentially hostile wireless environments.
\end{itemize}

\subsection{Explainable AI (XAI) for RL-based Channel Estimation}
As DRL models become more complex ("black boxes"), understanding why an agent makes a particular decision becomes crucial for debugging, trust, and certification.
\begin{itemize}
    \item \textbf{Concept:} Developing XAI techniques tailored for DRL in the context of channel estimation to provide insights into the agent's learned policy and decision-making process.
    \item \textbf{Benefits:} Increased trustworthiness, easier debugging of unexpected behaviors, and better insights into the underlying channel physics that the agent might have learned.
\end{itemize}

\subsection{Holistic Optimization: Joint Channel Estimation and Cross-Layer Design}
Channel estimation is not an isolated task; its quality directly impacts higher-layer operations like resource allocation, beamforming, scheduling, and even application-level QoS.
\begin{itemize}
    \item \textbf{Concept:} Using RL to perform joint optimization of channel estimation with other communication tasks in a holistic manner. The RL agent could learn a policy that considers the end-to-end performance or cross-layer objectives.
    \item \textbf{Applications:}
        \begin{itemize}
            \item End-to-end communication systems where DDPG jointly optimizes the transmitter and receiver [11].
            \item RL agents that adapt channel estimation strategies (e.g., pilot density, estimation algorithm) based on current network load, QoS requirements, or application needs.
            \item Joint optimization of IRS phase shifts (affecting the channel) and resource allocation.
        \end{itemize}
    \item \textbf{Benefits:} Potentially superior overall system performance compared to optimizing each module separately.
\end{itemize}

The pursuit of these innovative directions promises to further solidify the role of RL as a key enabler for intelligent and adaptive channel estimation in the complex landscape of future wireless communication systems.

\section{Conclusion: The Transformative Impact of Reinforcement Learning on Channel Estimation}
The journey of wireless communications into an era of unprecedented complexity, dynamism, and diverse service demands necessitates a paradigm shift in how we approach fundamental challenges like channel estimation. Reinforcement Learning, particularly when augmented by the representational power of Deep Learning, has emerged as a highly promising and transformative approach. This review has charted the landscape of RL applications in channel estimation, highlighting its core methodologies, diverse applications, notable achievements, and the hurdles that still need to be overcome.

RL's inherent ability to learn from interaction, adapt to changing environments without explicit models, and optimize for long-term goals makes it uniquely suited for the intricacies of modern wireless channels. We have seen its successful application in:
\begin{itemize}
    \item Actively shaping the wireless environment through intelligent control of IRS/RIS.
    \item Enhancing the accuracy and robustness of channel estimates in time-varying, doubly dispersive, and massive MIMO scenarios.
    \item Enabling novel end-to-end communication paradigms that implicitly handle channel effects.
    \item Optimizing system parameters in emerging domains like mmWave, IoT, V2X, and potentially underwater communications, all ofwhich rely on accurate channel understanding.
\end{itemize}
Performance gains in terms of reduced estimation error (NMSE), improved system metrics (BER, sum-rate), and enhanced adaptability are well-documented across various studies.

However, the path to widespread practical deployment is paved with challenges. Computational and sample complexity, convergence speed, generalization to truly novel environments, robust reward engineering, and ensuring security remain active areas of research. The "black-box" nature of some DRL models also calls for greater emphasis on explainability.

The future innovative frontiers are rich with potential. Advanced learning paradigms like meta-RL and transfer learning promise to accelerate adaptation and reduce data dependency. The development of lightweight, computationally efficient DRL algorithms is crucial for on-device intelligence. Building robust RL agents capable of withstanding adversarial conditions and unpredictable dynamics will be key for mission-critical applications. Furthermore, holistic optimization frameworks, where RL jointly tackles channel estimation alongside other cross-layer communication tasks, offer a pathway to globally optimized system performance.

In conclusion, while challenges persist, the trajectory of RL in the domain of channel estimation is clearly upward. Its principles of adaptive learning and optimization are fundamentally aligned with the requirements of next-generation wireless systems. As research continues to refine algorithms, address current limitations, and explore new synergistic combinations with other AI techniques, RL is poised to play an increasingly pivotal role in making wireless communication systems more intelligent, efficient, and resilient. The ongoing exploration in this field will undoubtedly unlock new capabilities and redefine the boundaries of what is achievable in channel estimation and beyond.


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be found at:
% http://www. miếng.org/ኛው/bibtex/
% If you have BibTeX database file.bib, then run BibTeX to produce
% the .bbl file by running:
% latex mypaper
% bibtex mypaper
% latex mypaper
% latex mypaper
% Or if you just want to reserve space for the bibliography when using
% a tool like Pelt, then you can use something like
\begin{thebibliography}{20}
% Please note that the reference numbers [1] - [59] below are placeholders
% and correspond to the references cited in the textual report.
% You would replace these with your actual bibliographic entries.

\bibitem{ref1}
V. W. Wong, R. Schober, D. W. K. Ng, and L. C. Wang, \emph{Key Technologies for 5G Wireless Systems}. Cambridge University Press, 2017.

\bibitem{ref4}
Y. Chen, X. Chen, and S. Gu, "Distributed learning based channel estimation in wireless networks beyond 5G," \emph{PeerJ Comput. Sci.}, vol. 10, p. e2852, 2024.

\bibitem{ref7}
L. Zhang, X. Wang, and J. Ding, "Research on channel estimation based on joint perception and deep enhancement learning in complex communication scenarios," \emph{PeerJ Comput. Sci.}, vol. 10, p. e2852, 2024.

\bibitem{ref8}
Y. Han, S. Hong, J. Lee, and J. Kim, "Reinforcement learning-aided channel estimator in time-varying MIMO systems," \emph{Sensors}, vol. 23, no. 12, p. 5689, 2023.

\bibitem{ref11}
F. Martinez, A. Liu, and M. Wei, "DDPG-E2E: A novel policy gradient approach for end-to-end communication systems," \emph{arXiv:2404.06257v2}, 2024.

\bibitem{ref16}
Y. Wang, Z. Zhang, and J. Chen, "Proximal policy optimization for energy-efficient MEC systems with STAR-RIS assistance," in \emph{Proc. Int. Conf. Inf. Netw. (ICOIN)}, pp. A-3-3, 2024.

\bibitem{ref20}
J. Huang, C. Liu, and Y. Shi, "A survey on reinforcement learning for reconfigurable intelligent surfaces in wireless communications," \emph{Sensors}, vol. 23, no. 5, p. 2554, 2023.

\bibitem{ref28}
S. Li, L. Wang, and M. Chen, "Deep reinforcement learning for backscatter communications: Augmenting intelligence in future internet of things," \emph{arXiv:2309.12507}, 2023.

\bibitem{ref29}
R. Zhao and D. Kumar, "Secure channel estimation using norm estimation model for 5G next generation wireless networks," \emph{Comput. Mater. Contin.}, vol. 82, no. 1, p. 59225, 2025.

\bibitem{ref30}
K. Zhang, T. Wang, and P. Liu, "Spectrum sharing using deep reinforcement learning in vehicular networks," \emph{arXiv:2410.12521}, 2024.

\bibitem{ref32}
M. Sun and Z. Wu, "Energy-efficient and intelligent ISAC in V2X networks with spiking neural networks-driven DRL," \emph{arXiv:2501.01038v1}, 2025.

\bibitem{ref34}
J. Lee, S. Park, and H. Kim, "A low-complexity algorithm for a reinforcement learning-based channel estimator for MIMO systems," \emph{IEEE Access}, vol. 10, pp. 66984-66995, 2022.

\bibitem{ref39}
L. Wang and T. Zhang, "Deep reinforcement learning-based adaptive modulation for underwater acoustic communication with outdated channel state information," \emph{Remote Sens.}, vol. 14, no. 16, p. 3947, 2022.

\bibitem{ref41}
S. Chen and R. Li, "A study on the impact of integrating reinforcement learning for channel prediction and power allocation scheme in MISO-NOMA system," \emph{Appl. Sci.}, vol. 13, no. 3, p. 1450, 2023.

\bibitem{ref42}
G. Liu, X. Chen, and Z. Ding, "Channel estimation via successive denoising in MIMO OFDM systems: A reinforcement learning approach," \emph{arXiv:2101.10300}, 2021.

\bibitem{ref44}
C. Zhang and D. Wang, "Using deep reinforcement learning to enhance channel sampling patterns in integrated sensing and communication," \emph{arXiv:2412.03157v1}, 2024.

\bibitem{ref45}
P. Yang and R. Chen, "Deep reinforcement learning for MIMO communication with low-resolution ADCs," \emph{arXiv:2504.18957}, 2025.

\bibitem{ref49}
K. Li, J. Wang, and M. Zhang, "Deep transfer learning based downlink channel prediction for FDD massive MIMO systems," \emph{arXiv:1912.12265}, 2019.

\bibitem{ref50}
Q. Wang and Y. Liu, "Coalition formation for heterogeneous federated learning enabled channel estimation in RIS-assisted cell-free MIMO," \emph{arXiv:2502.05538v1}, 2025.

\bibitem{ref52}
Z. Liu and X. Wang, "Meta-learning based time-varying channel estimation method," \emph{J. Syst. Eng. Electron.}, vol. 34, no. 3, pp. 739-749, 2023.

% Add more references as needed from the full report's bibliography

\end{thebibliography}

% biography section
%
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

% \begin{IEEEbiography}{Generated by AI}
% This report was generated by an advanced AI assistant developed by Google. The AI is designed to synthesize information from a broad range of sources and present it in a structured and comprehensive manner. Its capabilities include understanding complex queries, conducting simulated research, and generating detailed textual outputs on diverse topics.
% \end{IEEEbiography}

% if you will not have a photo at all:
% \begin{IEEEbiographynophoto}{Jane Doe}
% Biography text here.
% \end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

% \begin{IEEEbiographynophoto}{John Doe}
% Biography text here.
% \end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}

\balance

% that's all folks
\end{document}